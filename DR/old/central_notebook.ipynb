{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "from keras_efficientnets import EfficientNetB5\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration( gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=15500)])\n",
    "#Tesla P100-PCIE-16GB\n",
    "\n",
    "def load_img(img,lab):\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img,tf.float32) ##???\n",
    "    lab = tf.cast(lab,tf.float32)/2 #####???    turn 0,2,4 to 0,1,2 otherwise loss=NaN\n",
    "    return img,lab\n",
    "\n",
    "test_data=pd.read_csv('test.txt',header=None,sep=' ',names=['picture','label'])\n",
    "train_data=pd.read_csv('train.txt',header=None,sep=' ',names=['picture','label'])\n",
    "valid_data=pd.read_csv('valid.txt',header=None,sep=' ',names=['picture','label'])\n",
    "\n",
    "test_data['dir']=['preprocessed/'+'test/'+pic for pic in test_data['picture']]\n",
    "train_data['dir']=['preprocessed/'+'train/'+pic for pic in train_data['picture']]\n",
    "valid_data['dir']=['preprocessed/'+'valid/'+pic for pic in valid_data['picture']]\n",
    "\n",
    "total_set = pd.concat([test_data,train_data,valid_data],ignore_index=True)\n",
    "iid_total_set = pd.concat([total_set[total_set['label']==i*2][:1000] for i in range(3)])\n",
    "\n",
    "random_set = iid_total_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "## settings\n",
    "fraction_val = 0.2 ############################################################################\n",
    "\n",
    "valid_LEN=int(len(random_set)*fraction_val)\n",
    "train_LEN=int(len(random_set)-valid_LEN)\n",
    "\n",
    "batch_SIZE=5 ####################################################################################\n",
    "train_EPO=int(train_LEN // batch_SIZE)\n",
    "valid_EPO=int(valid_LEN // batch_SIZE)\n",
    "\n",
    "iid_valid = random_set[:valid_LEN]\n",
    "iid_train = random_set[valid_LEN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SET = tf.data.Dataset.from_tensor_slices((iid_train['dir'],iid_train['label'])).\\\n",
    "                            shuffle(train_LEN).\\\n",
    "                            map(load_img).\\\n",
    "                            batch(batch_SIZE).\\\n",
    "                            repeat().\\\n",
    "                            prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_SET = tf.data.Dataset.from_tensor_slices((iid_valid['dir'],iid_valid['label'])).\\\n",
    "                            shuffle(valid_LEN).\\\n",
    "                            map(load_img).\\\n",
    "                            batch(batch_SIZE).\\\n",
    "                            repeat().\\\n",
    "                            prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet = EfficientNetB5(weights='imagenet',\n",
    "                        include_top=False,\n",
    "                        input_shape=(456, 456, 3))\n",
    "effnet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def build_model(effnet):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(effnet)\n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(5, activation='relu'))\n",
    "    model.add(keras.layers.Dense(3))\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = build_model(effnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model._get_trainable_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo=5\n",
    "history= model.fit(np.array(train_SET),\n",
    "                  epochs=epo,\n",
    "                  steps_per_epoch=train_EPO,\n",
    "                  validation_data=valid_SET,\n",
    "                  validation_steps=valid_EPO\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model._get_trainable_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo=10\n",
    "history= model.fit(train_SET,\n",
    "                  epochs=epo,\n",
    "                  steps_per_epoch=train_EPO,\n",
    "                  validation_data=valid_SET,\n",
    "                  validation_steps=valid_EPO\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history.get('acc'), label='acc')\n",
    "plt.plot(history.epoch, history.history.get('val_acc'), label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history.get('loss'), label='loss')\n",
    "plt.plot(history.epoch, history.history.get('val_loss'), label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('myXception.h5')\n",
    "\n",
    "# acc = history.history.get('acc')\n",
    "# val_acc = history.history.get('val_acc')\n",
    "# loss = history.history.get('loss')\n",
    "# val_loss = history.history.get('val_loss')\n",
    "\n",
    "\n",
    "# np.save(\"acc.npy\", acc)\n",
    "# np.save(\"val_acc.npy\", val_acc)\n",
    "# np.save(\"loss.npy\", loss)\n",
    "# np.save(\"val_loss.npy\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
